2020-06-12 03:41:20,466 - INFO - logger set up
2020-06-12 03:41:20,466 - INFO - seed=718
2020-06-12 03:41:20,466 - INFO - #####
2020-06-12 03:41:20,467 - INFO - #####
2020-06-12 03:41:20,467 - INFO - Starting fold 0 ...
2020-06-12 03:41:20,467 - INFO - #####
2020-06-12 03:41:20,467 - INFO - #####
2020-06-12 03:41:20,617 - INFO - [load csv data] done in 0.15 s
2020-06-12 03:41:20,671 - INFO - [prepare validation data] done in 0.05 s
2020-06-12 03:41:21,779 - INFO - loading configuration file inputs/roberta-base/config.json
2020-06-12 03:41:21,779 - INFO - Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}

2020-06-12 03:41:21,780 - INFO - loading weights file inputs/roberta-base/pytorch_model.bin
2020-06-12 03:41:27,783 - INFO - [create model] done in 7.11 s
2020-06-12 03:41:27,783 - INFO - Starting 1 epoch...
2020-06-12 03:53:17,002 - INFO - save model at score=0.6652870735207974 on epoch=1
2020-06-12 03:53:17,003 - INFO - Starting 2 epoch...
2020-06-12 04:05:03,822 - INFO - save model at score=0.6923575403531155 on epoch=2
2020-06-12 04:05:03,823 - INFO - Starting 3 epoch...
2020-06-12 04:16:49,326 - INFO - min loss is not updated while 1 epochs of training
2020-06-12 04:16:49,326 - INFO - Starting 4 epoch...
2020-06-12 04:28:34,866 - INFO - min loss is not updated while 2 epochs of training
2020-06-12 04:28:34,866 - INFO - Starting 5 epoch...
2020-06-12 04:36:06,473 - INFO - logger set up
2020-06-12 04:36:06,473 - INFO - seed=718
2020-06-12 04:36:06,473 - INFO - #####
2020-06-12 04:36:06,473 - INFO - #####
2020-06-12 04:36:06,473 - INFO - Starting fold 0 ...
2020-06-12 04:36:06,473 - INFO - #####
2020-06-12 04:36:06,474 - INFO - #####
2020-06-12 04:36:06,625 - INFO - [load csv data] done in 0.15 s
2020-06-12 04:36:06,679 - INFO - [prepare validation data] done in 0.05 s
2020-06-12 04:36:07,783 - INFO - loading configuration file inputs/roberta-base/config.json
2020-06-12 04:36:07,784 - INFO - Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}

2020-06-12 04:36:07,784 - INFO - loading weights file inputs/roberta-base/pytorch_model.bin
2020-06-12 04:36:13,791 - INFO - [create model] done in 7.11 s
2020-06-12 04:36:13,791 - INFO - Starting 1 epoch...
2020-06-12 04:47:59,774 - INFO - save model at score=0.6657720983783214 on epoch=1
2020-06-12 04:47:59,774 - INFO - Starting 2 epoch...
2020-06-12 04:59:42,363 - INFO - save model at score=0.6922665981923302 on epoch=2
2020-06-12 04:59:42,363 - INFO - Starting 3 epoch...
2020-06-12 05:06:40,420 - INFO - logger set up
2020-06-12 05:06:40,420 - INFO - seed=718
2020-06-12 05:06:40,420 - INFO - #####
2020-06-12 05:06:40,420 - INFO - #####
2020-06-12 05:06:40,420 - INFO - Starting fold 0 ...
2020-06-12 05:06:40,420 - INFO - #####
2020-06-12 05:06:40,420 - INFO - #####
2020-06-12 05:06:40,577 - INFO - [load csv data] done in 0.16 s
2020-06-12 05:06:40,636 - INFO - [prepare validation data] done in 0.06 s
2020-06-12 05:06:41,747 - INFO - loading configuration file inputs/roberta-base/config.json
2020-06-12 05:06:41,747 - INFO - Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}

2020-06-12 05:06:41,748 - INFO - loading weights file inputs/roberta-base/pytorch_model.bin
2020-06-12 05:06:47,772 - INFO - [create model] done in 7.14 s
2020-06-12 05:06:47,773 - INFO - Starting 1 epoch...
2020-06-12 05:12:45,699 - INFO - save model at score=0.6755261970045704 on epoch=1
2020-06-12 05:12:45,700 - INFO - Starting 2 epoch...
2020-06-12 05:18:47,518 - INFO - save model at score=0.6923691677035022 on epoch=2
2020-06-12 05:18:47,518 - INFO - Starting 3 epoch...
2020-06-12 05:24:48,397 - INFO - min loss is not updated while 1 epochs of training
2020-06-12 05:24:48,397 - INFO - Starting 4 epoch...
2020-06-12 05:27:38,233 - INFO - logger set up
2020-06-12 05:27:38,233 - INFO - seed=718
2020-06-12 05:27:38,233 - INFO - #####
2020-06-12 05:27:38,233 - INFO - #####
2020-06-12 05:27:38,234 - INFO - Starting fold 0 ...
2020-06-12 05:27:38,234 - INFO - #####
2020-06-12 05:27:38,234 - INFO - #####
2020-06-12 05:27:38,386 - INFO - [load csv data] done in 0.15 s
2020-06-12 05:27:38,442 - INFO - [prepare validation data] done in 0.06 s
2020-06-12 05:27:39,015 - INFO - loading configuration file inputs/roberta-base/config.json
2020-06-12 05:27:39,016 - INFO - Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}

2020-06-12 05:27:39,016 - INFO - loading weights file inputs/roberta-base/pytorch_model.bin
2020-06-12 05:27:45,214 - INFO - [create model] done in 6.77 s
2020-06-12 05:27:45,214 - INFO - Starting 1 epoch...
2020-06-12 05:33:45,527 - INFO - save model at score=0.698148219647907 on epoch=1
2020-06-12 05:33:45,528 - INFO - Starting 2 epoch...
2020-06-12 05:39:46,147 - INFO - save model at score=0.7032790288899736 on epoch=2
2020-06-12 05:39:46,147 - INFO - Starting 3 epoch...
2020-06-12 05:45:45,858 - INFO - min loss is not updated while 1 epochs of training
2020-06-12 05:45:45,858 - INFO - Starting 4 epoch...
2020-06-12 05:51:44,724 - INFO - min loss is not updated while 2 epochs of training
2020-06-12 05:51:44,724 - INFO - Starting 5 epoch...
2020-06-12 05:52:37,508 - INFO - logger set up
2020-06-12 05:52:37,508 - INFO - seed=718
2020-06-12 05:52:37,508 - INFO - #####
2020-06-12 05:52:37,508 - INFO - #####
2020-06-12 05:52:37,508 - INFO - Starting fold 0 ...
2020-06-12 05:52:37,509 - INFO - #####
2020-06-12 05:52:37,509 - INFO - #####
2020-06-12 05:52:37,662 - INFO - [load csv data] done in 0.15 s
2020-06-12 05:52:37,718 - INFO - [prepare validation data] done in 0.06 s
2020-06-12 05:52:38,282 - INFO - loading configuration file inputs/roberta-base/config.json
2020-06-12 05:52:38,282 - INFO - Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}

2020-06-12 05:52:38,283 - INFO - loading weights file inputs/roberta-base/pytorch_model.bin
2020-06-12 05:52:44,472 - INFO - [create model] done in 6.75 s
2020-06-12 05:52:44,472 - INFO - Starting 1 epoch...
2020-06-12 05:58:46,335 - INFO - save model at score=0.6984513601838597 on epoch=1
2020-06-12 05:58:46,335 - INFO - Starting 2 epoch...
2020-06-12 06:01:33,610 - INFO - logger set up
2020-06-12 06:01:33,610 - INFO - seed=718
2020-06-12 06:01:33,610 - INFO - #####
2020-06-12 06:01:33,610 - INFO - #####
2020-06-12 06:01:33,610 - INFO - Starting fold 0 ...
2020-06-12 06:01:33,610 - INFO - #####
2020-06-12 06:01:33,610 - INFO - #####
2020-06-12 06:01:33,764 - INFO - [load csv data] done in 0.15 s
2020-06-12 06:01:33,818 - INFO - [prepare validation data] done in 0.05 s
2020-06-12 06:01:34,382 - INFO - loading configuration file inputs/roberta-base/config.json
2020-06-12 06:01:34,382 - INFO - Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}

2020-06-12 06:01:34,383 - INFO - loading weights file inputs/roberta-base/pytorch_model.bin
2020-06-12 06:01:40,540 - INFO - [create model] done in 6.72 s
2020-06-12 06:01:40,540 - INFO - Starting 1 epoch...
2020-06-12 06:15:04,684 - INFO - logger set up
2020-06-12 06:15:04,684 - INFO - seed=718
2020-06-12 06:15:04,684 - INFO - #####
2020-06-12 06:15:04,684 - INFO - #####
2020-06-12 06:15:04,684 - INFO - Starting fold 0 ...
2020-06-12 06:15:04,684 - INFO - #####
2020-06-12 06:15:04,685 - INFO - #####
2020-06-12 06:15:04,839 - INFO - [load csv data] done in 0.15 s
2020-06-12 06:15:04,894 - INFO - [prepare validation data] done in 0.05 s
2020-06-12 06:15:05,477 - INFO - loading configuration file inputs/roberta-base/config.json
2020-06-12 06:15:05,477 - INFO - Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}

2020-06-12 06:15:05,478 - INFO - loading weights file inputs/roberta-base/pytorch_model.bin
2020-06-12 06:15:11,875 - INFO - [create model] done in 6.98 s
2020-06-12 06:15:11,875 - INFO - Starting 1 epoch...
2020-06-12 06:21:11,421 - INFO - save model at score=0.4985711717088893 on epoch=1
2020-06-12 06:21:11,421 - INFO - Starting 2 epoch...
2020-06-12 06:28:21,943 - INFO - logger set up
2020-06-12 06:28:21,943 - INFO - seed=718
2020-06-12 06:28:21,943 - INFO - #####
2020-06-12 06:28:21,944 - INFO - #####
2020-06-12 06:28:21,944 - INFO - Starting fold 0 ...
2020-06-12 06:28:21,944 - INFO - #####
2020-06-12 06:28:21,944 - INFO - #####
2020-06-12 06:28:22,097 - INFO - [load csv data] done in 0.15 s
2020-06-12 06:28:22,153 - INFO - [prepare validation data] done in 0.06 s
2020-06-12 06:28:22,743 - INFO - loading configuration file inputs/roberta-base/config.json
2020-06-12 06:28:22,744 - INFO - Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}

2020-06-12 06:28:22,746 - INFO - loading weights file inputs/roberta-base/pytorch_model.bin
2020-06-12 06:28:28,908 - INFO - [create model] done in 6.76 s
2020-06-12 06:28:28,908 - INFO - Starting 1 epoch...
2020-06-12 06:29:43,780 - INFO - logger set up
2020-06-12 06:29:43,780 - INFO - seed=718
2020-06-12 06:29:43,780 - INFO - #####
2020-06-12 06:29:43,780 - INFO - #####
2020-06-12 06:29:43,781 - INFO - Starting fold 0 ...
2020-06-12 06:29:43,781 - INFO - #####
2020-06-12 06:29:43,781 - INFO - #####
2020-06-12 06:29:43,934 - INFO - [load csv data] done in 0.15 s
2020-06-12 06:29:43,989 - INFO - [prepare validation data] done in 0.05 s
2020-06-12 06:29:44,552 - INFO - loading configuration file inputs/roberta-base/config.json
2020-06-12 06:29:44,553 - INFO - Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}

2020-06-12 06:29:44,553 - INFO - loading weights file inputs/roberta-base/pytorch_model.bin
2020-06-12 06:29:50,622 - INFO - [create model] done in 6.63 s
2020-06-12 06:29:50,622 - INFO - Starting 1 epoch...
2020-06-12 06:35:44,248 - INFO - logger set up
2020-06-12 06:35:44,248 - INFO - seed=718
2020-06-12 06:35:44,248 - INFO - #####
2020-06-12 06:35:44,248 - INFO - #####
2020-06-12 06:35:44,249 - INFO - Starting fold 0 ...
2020-06-12 06:35:44,249 - INFO - #####
2020-06-12 06:35:44,249 - INFO - #####
2020-06-12 06:35:44,399 - INFO - [load csv data] done in 0.15 s
2020-06-12 06:35:44,453 - INFO - [prepare validation data] done in 0.05 s
2020-06-12 06:35:45,564 - INFO - loading configuration file inputs/roberta-base/config.json
2020-06-12 06:35:45,564 - INFO - Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}

2020-06-12 06:35:45,565 - INFO - loading weights file inputs/roberta-base/pytorch_model.bin
2020-06-12 06:35:51,586 - INFO - [create model] done in 7.13 s
2020-06-12 06:35:51,586 - INFO - Starting 1 epoch...
2020-06-12 06:41:43,524 - INFO - save model at score=0.5100739895528436 on epoch=1
2020-06-12 06:41:43,524 - INFO - Starting 2 epoch...
2020-06-12 06:43:41,845 - INFO - logger set up
2020-06-12 06:43:41,845 - INFO - seed=718
2020-06-12 06:43:41,845 - INFO - #####
2020-06-12 06:43:41,846 - INFO - #####
2020-06-12 06:43:41,846 - INFO - Starting fold 0 ...
2020-06-12 06:43:41,846 - INFO - #####
2020-06-12 06:43:41,846 - INFO - #####
2020-06-12 06:43:41,997 - INFO - [load csv data] done in 0.15 s
2020-06-12 06:43:42,051 - INFO - [prepare validation data] done in 0.05 s
2020-06-12 06:43:43,164 - INFO - loading configuration file inputs/roberta-base/config.json
2020-06-12 06:43:43,165 - INFO - Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}

2020-06-12 06:43:43,165 - INFO - loading weights file inputs/roberta-base/pytorch_model.bin
2020-06-12 06:43:49,146 - INFO - [create model] done in 7.09 s
2020-06-12 06:43:49,146 - INFO - Starting 1 epoch...
2020-06-12 06:49:40,545 - INFO - save model at score=0.687944028887461 on epoch=1
2020-06-12 06:49:40,545 - INFO - Starting 2 epoch...
2020-06-12 06:55:30,501 - INFO - min loss is not updated while 1 epochs of training
2020-06-12 06:55:30,502 - INFO - Starting 3 epoch...
2020-06-12 07:01:20,129 - INFO - min loss is not updated while 2 epochs of training
2020-06-12 07:01:20,130 - INFO - Starting 4 epoch...
2020-06-12 07:07:09,606 - INFO - min loss is not updated while 3 epochs of training
2020-06-12 07:07:09,607 - INFO - Early Stopping
2020-06-12 07:07:09,607 - INFO - best score=0.687944028887461 on epoch=1
2020-06-12 07:07:09,607 - INFO - [training loop] done in 1400.46 s
2020-06-12 07:07:09,609 - INFO - #####
2020-06-12 07:07:09,609 - INFO - #####
2020-06-12 07:07:09,609 - INFO - Starting fold 1 ...
2020-06-12 07:07:09,610 - INFO - #####
2020-06-12 07:07:09,610 - INFO - #####
2020-06-12 07:07:09,764 - INFO - [load csv data] done in 0.15 s
2020-06-12 07:07:09,817 - INFO - [prepare validation data] done in 0.05 s
2020-06-12 07:07:09,817 - INFO - loading configuration file inputs/roberta-base/config.json
2020-06-12 07:07:09,818 - INFO - Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}

2020-06-12 07:07:09,818 - INFO - loading weights file inputs/roberta-base/pytorch_model.bin
2020-06-12 07:07:12,205 - INFO - [create model] done in 2.39 s
2020-06-12 07:07:12,205 - INFO - Starting 1 epoch...
2020-06-12 07:13:02,597 - INFO - save model at score=0.6977355806987452 on epoch=1
2020-06-12 07:13:02,597 - INFO - Starting 2 epoch...
2020-06-12 07:18:52,403 - INFO - save model at score=0.7058011535258034 on epoch=2
2020-06-12 07:18:52,403 - INFO - Starting 3 epoch...
2020-06-12 07:24:41,413 - INFO - min loss is not updated while 1 epochs of training
2020-06-12 07:24:41,413 - INFO - Starting 4 epoch...
2020-06-12 07:30:30,753 - INFO - min loss is not updated while 2 epochs of training
2020-06-12 07:30:30,753 - INFO - Starting 5 epoch...
2020-06-12 07:36:20,497 - INFO - min loss is not updated while 3 epochs of training
2020-06-12 07:36:20,497 - INFO - Early Stopping
2020-06-12 07:36:20,497 - INFO - best score=0.7058011535258034 on epoch=2
2020-06-12 07:36:20,497 - INFO - [training loop] done in 1748.29 s
2020-06-12 07:36:20,500 - INFO - #####
2020-06-12 07:36:20,500 - INFO - #####
2020-06-12 07:36:20,500 - INFO - Starting fold 2 ...
2020-06-12 07:36:20,500 - INFO - #####
2020-06-12 07:36:20,500 - INFO - #####
2020-06-12 07:36:20,643 - INFO - [load csv data] done in 0.14 s
2020-06-12 07:36:20,698 - INFO - [prepare validation data] done in 0.05 s
2020-06-12 07:36:20,698 - INFO - loading configuration file inputs/roberta-base/config.json
2020-06-12 07:36:20,698 - INFO - Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}

2020-06-12 07:36:20,699 - INFO - loading weights file inputs/roberta-base/pytorch_model.bin
2020-06-12 07:36:22,900 - INFO - [create model] done in 2.2 s
2020-06-12 07:36:22,901 - INFO - Starting 1 epoch...
2020-06-12 07:42:13,026 - INFO - save model at score=0.6934890346955013 on epoch=1
2020-06-12 07:42:13,026 - INFO - Starting 2 epoch...
2020-06-12 07:48:02,746 - INFO - min loss is not updated while 1 epochs of training
2020-06-12 07:48:02,747 - INFO - Starting 3 epoch...
2020-06-12 07:53:53,089 - INFO - min loss is not updated while 2 epochs of training
2020-06-12 07:53:53,089 - INFO - Starting 4 epoch...
2020-06-12 07:59:52,587 - INFO - min loss is not updated while 3 epochs of training
2020-06-12 07:59:52,588 - INFO - Early Stopping
2020-06-12 07:59:52,588 - INFO - best score=0.6934890346955013 on epoch=1
2020-06-12 07:59:52,588 - INFO - [training loop] done in 1409.69 s
2020-06-12 07:59:52,591 - INFO - #####
2020-06-12 07:59:52,591 - INFO - #####
2020-06-12 07:59:52,591 - INFO - Starting fold 3 ...
2020-06-12 07:59:52,592 - INFO - #####
2020-06-12 07:59:52,592 - INFO - #####
2020-06-12 07:59:52,751 - INFO - [load csv data] done in 0.16 s
2020-06-12 07:59:52,810 - INFO - [prepare validation data] done in 0.06 s
2020-06-12 07:59:52,810 - INFO - loading configuration file inputs/roberta-base/config.json
2020-06-12 07:59:52,811 - INFO - Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}

2020-06-12 07:59:52,811 - INFO - loading weights file inputs/roberta-base/pytorch_model.bin
2020-06-12 07:59:55,018 - INFO - [create model] done in 2.21 s
2020-06-12 07:59:55,018 - INFO - Starting 1 epoch...
2020-06-12 08:05:53,796 - INFO - save model at score=0.6968645216933569 on epoch=1
2020-06-12 08:05:53,797 - INFO - Starting 2 epoch...
2020-06-12 08:11:52,679 - INFO - save model at score=0.7093133580983462 on epoch=2
2020-06-12 08:11:52,679 - INFO - Starting 3 epoch...
2020-06-12 08:17:53,557 - INFO - min loss is not updated while 1 epochs of training
2020-06-12 08:17:53,557 - INFO - Starting 4 epoch...
2020-06-12 08:23:53,569 - INFO - min loss is not updated while 2 epochs of training
2020-06-12 08:23:53,569 - INFO - Starting 5 epoch...
2020-06-12 08:29:53,021 - INFO - min loss is not updated while 3 epochs of training
2020-06-12 08:29:53,021 - INFO - Early Stopping
2020-06-12 08:29:53,021 - INFO - best score=0.7093133580983462 on epoch=2
2020-06-12 08:29:53,021 - INFO - [training loop] done in 1798.0 s
2020-06-12 08:29:53,023 - INFO - #####
2020-06-12 08:29:53,023 - INFO - #####
2020-06-12 08:29:53,023 - INFO - Starting fold 4 ...
2020-06-12 08:29:53,024 - INFO - #####
2020-06-12 08:29:53,024 - INFO - #####
2020-06-12 08:29:53,183 - INFO - [load csv data] done in 0.16 s
2020-06-12 08:29:53,239 - INFO - [prepare validation data] done in 0.06 s
2020-06-12 08:29:53,240 - INFO - loading configuration file inputs/roberta-base/config.json
2020-06-12 08:29:53,240 - INFO - Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}

2020-06-12 08:29:53,240 - INFO - loading weights file inputs/roberta-base/pytorch_model.bin
2020-06-12 08:29:55,489 - INFO - [create model] done in 2.25 s
2020-06-12 08:29:55,489 - INFO - Starting 1 epoch...
2020-06-12 08:35:55,519 - INFO - save model at score=0.6948501617690632 on epoch=1
2020-06-12 08:35:55,519 - INFO - Starting 2 epoch...
2020-06-12 08:41:54,536 - INFO - min loss is not updated while 1 epochs of training
2020-06-12 08:41:54,537 - INFO - Starting 3 epoch...
2020-06-12 08:47:54,463 - INFO - save model at score=0.7016069215915554 on epoch=3
2020-06-12 08:47:54,463 - INFO - Starting 4 epoch...
2020-06-12 08:53:54,222 - INFO - min loss is not updated while 1 epochs of training
2020-06-12 08:53:54,223 - INFO - Starting 5 epoch...
2020-06-12 08:59:54,670 - INFO - min loss is not updated while 2 epochs of training
2020-06-12 08:59:54,671 - INFO - best score=0.7016069215915554 on epoch=3
2020-06-12 08:59:54,671 - INFO - [training loop] done in 1799.18 s
2020-06-12 11:08:28,135 - INFO - logger set up
2020-06-12 11:08:28,135 - INFO - seed=718
2020-06-12 11:08:28,136 - INFO - #####
2020-06-12 11:08:28,136 - INFO - #####
2020-06-12 11:08:28,136 - INFO - Starting fold 0 ...
2020-06-12 11:08:28,136 - INFO - #####
2020-06-12 11:08:28,136 - INFO - #####
2020-06-12 11:08:28,290 - INFO - [load csv data] done in 0.15 s
2020-06-12 11:08:28,344 - INFO - [prepare validation data] done in 0.05 s
2020-06-12 11:08:28,907 - INFO - loading configuration file inputs/roberta-base/config.json
2020-06-12 11:08:28,908 - INFO - Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}

2020-06-12 11:08:28,908 - INFO - loading weights file inputs/roberta-base/pytorch_model.bin
2020-06-12 11:08:35,017 - INFO - [create model] done in 6.67 s
2020-06-12 11:08:35,017 - INFO - Starting 1 epoch...
2020-06-12 11:14:34,563 - INFO - save model at score=0.689022559608589 on epoch=1
2020-06-12 11:14:34,563 - INFO - Starting 2 epoch...
2020-06-12 11:18:51,465 - INFO - logger set up
2020-06-12 11:18:51,465 - INFO - seed=718
2020-06-12 11:18:51,465 - INFO - #####
2020-06-12 11:18:51,465 - INFO - #####
2020-06-12 11:18:51,466 - INFO - Starting fold 0 ...
2020-06-12 11:18:51,466 - INFO - #####
2020-06-12 11:18:51,466 - INFO - #####
2020-06-12 11:18:51,619 - INFO - [load csv data] done in 0.15 s
2020-06-12 11:18:51,674 - INFO - [prepare validation data] done in 0.05 s
2020-06-12 11:18:52,237 - INFO - loading configuration file inputs/roberta-base/config.json
2020-06-12 11:18:52,237 - INFO - Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}

2020-06-12 11:18:52,238 - INFO - loading weights file inputs/roberta-base/pytorch_model.bin
2020-06-12 11:18:58,359 - INFO - [create model] done in 6.69 s
2020-06-12 11:18:58,360 - INFO - Starting 1 epoch...
2020-06-12 11:19:34,767 - INFO - logger set up
2020-06-12 11:19:34,768 - INFO - seed=718
2020-06-12 11:19:34,768 - INFO - #####
2020-06-12 11:19:34,768 - INFO - #####
2020-06-12 11:19:34,768 - INFO - Starting fold 0 ...
2020-06-12 11:19:34,768 - INFO - #####
2020-06-12 11:19:34,768 - INFO - #####
2020-06-12 11:19:34,922 - INFO - [load csv data] done in 0.15 s
2020-06-12 11:19:34,977 - INFO - [prepare validation data] done in 0.06 s
2020-06-12 11:19:35,542 - INFO - loading configuration file inputs/roberta-base/config.json
2020-06-12 11:19:35,542 - INFO - Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}

2020-06-12 11:19:35,543 - INFO - loading weights file inputs/roberta-base/pytorch_model.bin
2020-06-12 11:19:41,677 - INFO - [create model] done in 6.7 s
2020-06-12 11:19:41,677 - INFO - Starting 1 epoch...
2020-06-12 11:20:47,974 - INFO - logger set up
2020-06-12 11:20:47,974 - INFO - seed=718
2020-06-12 11:20:47,974 - INFO - #####
2020-06-12 11:20:47,975 - INFO - #####
2020-06-12 11:20:47,975 - INFO - Starting fold 0 ...
2020-06-12 11:20:47,975 - INFO - #####
2020-06-12 11:20:47,975 - INFO - #####
2020-06-12 11:20:48,146 - INFO - [load csv data] done in 0.17 s
2020-06-12 11:20:48,201 - INFO - [prepare validation data] done in 0.06 s
2020-06-12 11:20:48,801 - INFO - loading configuration file inputs/roberta-base/config.json
2020-06-12 11:20:48,802 - INFO - Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}

2020-06-12 11:20:48,803 - INFO - loading weights file inputs/roberta-base/pytorch_model.bin
2020-06-12 11:20:55,111 - INFO - [create model] done in 6.91 s
2020-06-12 11:20:55,111 - INFO - Starting 1 epoch...
2020-06-12 11:39:28,346 - INFO - logger set up
2020-06-12 11:39:28,346 - INFO - seed=718
2020-06-12 11:39:28,346 - INFO - #####
2020-06-12 11:39:28,346 - INFO - #####
2020-06-12 11:39:28,346 - INFO - Starting fold 0 ...
2020-06-12 11:39:28,347 - INFO - #####
2020-06-12 11:39:28,347 - INFO - #####
2020-06-12 11:39:28,502 - INFO - [load csv data] done in 0.16 s
2020-06-12 11:39:28,558 - INFO - [prepare validation data] done in 0.06 s
2020-06-12 11:39:29,120 - INFO - loading configuration file inputs/roberta-base/config.json
2020-06-12 11:39:29,121 - INFO - Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}

2020-06-12 11:39:29,122 - INFO - loading weights file inputs/roberta-base/pytorch_model.bin
2020-06-12 11:39:35,227 - INFO - [create model] done in 6.67 s
2020-06-12 11:39:35,227 - INFO - Starting 1 epoch...
2020-06-12 11:41:03,169 - INFO - logger set up
2020-06-12 11:41:03,169 - INFO - seed=718
2020-06-12 11:41:03,169 - INFO - #####
2020-06-12 11:41:03,169 - INFO - #####
2020-06-12 11:41:03,169 - INFO - Starting fold 0 ...
2020-06-12 11:41:03,169 - INFO - #####
2020-06-12 11:41:03,170 - INFO - #####
2020-06-12 11:41:03,323 - INFO - [load csv data] done in 0.15 s
2020-06-12 11:41:03,377 - INFO - [prepare validation data] done in 0.05 s
2020-06-12 11:41:03,941 - INFO - loading configuration file inputs/roberta-base/config.json
2020-06-12 11:41:03,942 - INFO - Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}

2020-06-12 11:41:03,943 - INFO - loading weights file inputs/roberta-base/pytorch_model.bin
2020-06-12 11:41:10,017 - INFO - [create model] done in 6.64 s
2020-06-12 11:41:10,017 - INFO - Starting 1 epoch...
2020-06-12 11:42:15,775 - INFO - logger set up
2020-06-12 11:42:15,775 - INFO - seed=718
2020-06-12 11:42:15,776 - INFO - #####
2020-06-12 11:42:15,776 - INFO - #####
2020-06-12 11:42:15,776 - INFO - Starting fold 0 ...
2020-06-12 11:42:15,776 - INFO - #####
2020-06-12 11:42:15,776 - INFO - #####
2020-06-12 11:42:15,927 - INFO - [load csv data] done in 0.15 s
2020-06-12 11:42:15,981 - INFO - [prepare validation data] done in 0.05 s
2020-06-12 11:42:16,544 - INFO - loading configuration file inputs/roberta-base/config.json
2020-06-12 11:42:16,544 - INFO - Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}

2020-06-12 11:42:16,545 - INFO - loading weights file inputs/roberta-base/pytorch_model.bin
2020-06-12 11:42:22,749 - INFO - [create model] done in 6.77 s
2020-06-12 11:42:22,749 - INFO - Starting 1 epoch...
2020-06-12 11:42:52,023 - INFO - logger set up
2020-06-12 11:42:52,023 - INFO - seed=718
2020-06-12 11:42:52,024 - INFO - #####
2020-06-12 11:42:52,024 - INFO - #####
2020-06-12 11:42:52,024 - INFO - Starting fold 0 ...
2020-06-12 11:42:52,024 - INFO - #####
2020-06-12 11:42:52,024 - INFO - #####
2020-06-12 11:42:52,176 - INFO - [load csv data] done in 0.15 s
2020-06-12 11:42:52,232 - INFO - [prepare validation data] done in 0.06 s
2020-06-12 11:42:52,794 - INFO - loading configuration file inputs/roberta-base/config.json
2020-06-12 11:42:52,794 - INFO - Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}

2020-06-12 11:42:52,795 - INFO - loading weights file inputs/roberta-base/pytorch_model.bin
2020-06-12 11:42:58,900 - INFO - [create model] done in 6.67 s
2020-06-12 11:42:58,900 - INFO - Starting 1 epoch...
2020-06-12 11:49:01,814 - INFO - save model at score=0.3766994292315647 on epoch=1
2020-06-12 11:49:01,814 - INFO - Starting 2 epoch...
2020-06-12 11:55:19,762 - INFO - logger set up
2020-06-12 11:55:19,762 - INFO - seed=718
2020-06-12 11:55:19,762 - INFO - #####
2020-06-12 11:55:19,762 - INFO - #####
2020-06-12 11:55:19,762 - INFO - Starting fold 0 ...
2020-06-12 11:55:19,762 - INFO - #####
2020-06-12 11:55:19,762 - INFO - #####
2020-06-12 11:55:19,916 - INFO - [load csv data] done in 0.15 s
2020-06-12 11:55:19,971 - INFO - [prepare validation data] done in 0.05 s
2020-06-12 11:55:20,560 - INFO - loading configuration file inputs/roberta-base/config.json
2020-06-12 11:55:20,561 - INFO - Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}

2020-06-12 11:55:20,563 - INFO - loading weights file inputs/roberta-base/pytorch_model.bin
2020-06-12 11:55:26,815 - INFO - [create model] done in 6.84 s
2020-06-12 11:55:26,815 - INFO - Starting 1 epoch...
2020-06-12 12:01:31,324 - INFO - save model at score=0.6937603365107579 on epoch=1
2020-06-12 12:01:31,324 - INFO - Starting 2 epoch...
2020-06-12 12:07:36,431 - INFO - min loss is not updated while 1 epochs of training
2020-06-12 12:07:36,431 - INFO - Starting 3 epoch...
2020-06-12 12:10:23,938 - INFO - logger set up
2020-06-12 12:10:23,938 - INFO - seed=718
2020-06-12 12:10:23,938 - INFO - #####
2020-06-12 12:10:23,938 - INFO - #####
2020-06-12 12:10:23,938 - INFO - Starting fold 0 ...
2020-06-12 12:10:23,938 - INFO - #####
2020-06-12 12:10:23,939 - INFO - #####
2020-06-12 12:10:24,092 - INFO - [load csv data] done in 0.15 s
2020-06-12 12:10:24,146 - INFO - [prepare validation data] done in 0.05 s
2020-06-12 12:10:24,709 - INFO - loading configuration file inputs/roberta-base/config.json
2020-06-12 12:10:24,709 - INFO - Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}

2020-06-12 12:10:24,710 - INFO - loading weights file inputs/roberta-base/pytorch_model.bin
2020-06-12 12:10:30,826 - INFO - [create model] done in 6.68 s
2020-06-12 12:10:30,826 - INFO - Starting 1 epoch...
2020-06-12 12:11:03,586 - INFO - logger set up
2020-06-12 12:11:03,586 - INFO - seed=718
2020-06-12 12:11:03,586 - INFO - #####
2020-06-12 12:11:03,586 - INFO - #####
2020-06-12 12:11:03,586 - INFO - Starting fold 0 ...
2020-06-12 12:11:03,586 - INFO - #####
2020-06-12 12:11:03,586 - INFO - #####
2020-06-12 12:11:03,741 - INFO - [load csv data] done in 0.15 s
2020-06-12 12:11:03,796 - INFO - [prepare validation data] done in 0.05 s
2020-06-12 12:11:04,360 - INFO - loading configuration file inputs/roberta-base/config.json
2020-06-12 12:11:04,361 - INFO - Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}

2020-06-12 12:11:04,361 - INFO - loading weights file inputs/roberta-base/pytorch_model.bin
2020-06-12 12:11:10,536 - INFO - [create model] done in 6.74 s
2020-06-12 12:11:10,536 - INFO - Starting 1 epoch...
2020-06-12 12:15:28,471 - INFO - save model at score=0.6905376552034765 on epoch=1
2020-06-12 12:15:28,471 - INFO - Starting 2 epoch...
2020-06-12 12:16:27,256 - INFO - logger set up
2020-06-12 12:16:27,256 - INFO - seed=718
2020-06-12 12:16:27,256 - INFO - #####
2020-06-12 12:16:27,256 - INFO - #####
2020-06-12 12:16:27,256 - INFO - Starting fold 0 ...
2020-06-12 12:16:27,256 - INFO - #####
2020-06-12 12:16:27,256 - INFO - #####
2020-06-12 12:16:27,409 - INFO - [load csv data] done in 0.15 s
2020-06-12 12:16:27,463 - INFO - [prepare validation data] done in 0.05 s
2020-06-12 12:16:28,026 - INFO - loading configuration file inputs/roberta-base/config.json
2020-06-12 12:16:28,026 - INFO - Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}

2020-06-12 12:16:28,027 - INFO - loading weights file inputs/roberta-base/pytorch_model.bin
2020-06-12 12:16:34,151 - INFO - [create model] done in 6.69 s
2020-06-12 12:16:34,151 - INFO - Starting 1 epoch...
2020-06-12 12:20:53,592 - INFO - save model at score=0.6905965433224093 on epoch=1
2020-06-12 12:20:53,592 - INFO - Starting 2 epoch...
2020-06-12 12:25:11,679 - INFO - save model at score=0.6972196331669956 on epoch=2
2020-06-12 12:25:11,679 - INFO - Starting 3 epoch...
2020-06-12 12:29:28,719 - INFO - min loss is not updated while 1 epochs of training
2020-06-12 12:29:28,719 - INFO - best score=0.6972196331669956 on epoch=2
2020-06-12 12:29:28,720 - INFO - [training loop] done in 774.57 s
2020-06-12 12:29:28,725 - INFO - #####
2020-06-12 12:29:28,725 - INFO - #####
2020-06-12 12:29:28,725 - INFO - Starting fold 1 ...
2020-06-12 12:29:28,725 - INFO - #####
2020-06-12 12:29:28,725 - INFO - #####
2020-06-12 12:29:28,974 - INFO - [load csv data] done in 0.25 s
2020-06-12 12:29:29,028 - INFO - [prepare validation data] done in 0.05 s
2020-06-12 12:29:29,028 - INFO - loading configuration file inputs/roberta-base/config.json
2020-06-12 12:29:29,029 - INFO - Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}

2020-06-12 12:29:29,029 - INFO - loading weights file inputs/roberta-base/pytorch_model.bin
2020-06-12 12:29:31,434 - INFO - [create model] done in 2.41 s
2020-06-12 12:29:31,435 - INFO - Starting 1 epoch...
2020-06-12 12:35:03,170 - INFO - logger set up
2020-06-12 12:35:03,171 - INFO - seed=718
2020-06-12 12:35:03,171 - INFO - #####
2020-06-12 12:35:03,171 - INFO - #####
2020-06-12 12:35:03,171 - INFO - Starting fold 0 ...
2020-06-12 12:35:03,171 - INFO - #####
2020-06-12 12:35:03,171 - INFO - #####
2020-06-12 12:35:03,325 - INFO - [load csv data] done in 0.15 s
2020-06-12 12:35:03,379 - INFO - [prepare validation data] done in 0.05 s
2020-06-12 12:35:03,942 - INFO - loading configuration file inputs/roberta-base/config.json
2020-06-12 12:35:03,943 - INFO - Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}

2020-06-12 12:35:03,944 - INFO - loading weights file inputs/roberta-base/pytorch_model.bin
2020-06-12 12:37:26,753 - INFO - logger set up
2020-06-12 12:37:26,753 - INFO - seed=718
2020-06-12 12:37:26,753 - INFO - #####
2020-06-12 12:37:26,753 - INFO - #####
2020-06-12 12:37:26,753 - INFO - Starting fold 0 ...
2020-06-12 12:37:26,753 - INFO - #####
2020-06-12 12:37:26,753 - INFO - #####
2020-06-12 12:37:26,912 - INFO - [load csv data] done in 0.16 s
2020-06-12 12:37:26,972 - INFO - [prepare validation data] done in 0.06 s
2020-06-12 12:37:27,536 - INFO - loading configuration file inputs/roberta-base/config.json
2020-06-12 12:37:27,537 - INFO - Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}

2020-06-12 12:37:27,537 - INFO - loading weights file inputs/roberta-base/pytorch_model.bin
2020-06-12 12:37:33,650 - INFO - [create model] done in 6.68 s
2020-06-12 12:37:33,650 - INFO - Starting 1 epoch...
2020-06-12 12:41:50,364 - INFO - save model at score=0.6861204619317001 on epoch=1
2020-06-12 12:41:50,364 - INFO - Starting 2 epoch...
2020-06-12 12:46:08,239 - INFO - save model at score=0.6973307696247577 on epoch=2
2020-06-12 12:46:08,240 - INFO - Starting 3 epoch...
2020-06-12 12:50:26,059 - INFO - save model at score=0.6976466691360906 on epoch=3
2020-06-12 12:50:26,059 - INFO - best score=0.6976466691360906 on epoch=3
2020-06-12 12:50:26,060 - INFO - [training loop] done in 772.41 s
2020-06-12 12:50:26,062 - INFO - #####
2020-06-12 12:50:26,062 - INFO - #####
2020-06-12 12:50:26,062 - INFO - Starting fold 1 ...
2020-06-12 12:50:26,062 - INFO - #####
2020-06-12 12:50:26,063 - INFO - #####
2020-06-12 12:50:26,230 - INFO - [load csv data] done in 0.17 s
2020-06-12 12:50:26,294 - INFO - [prepare validation data] done in 0.06 s
2020-06-12 12:50:26,294 - INFO - loading configuration file inputs/roberta-base/config.json
2020-06-12 12:50:26,295 - INFO - Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}

2020-06-12 12:50:26,295 - INFO - loading weights file inputs/roberta-base/pytorch_model.bin
2020-06-12 12:50:28,763 - INFO - [create model] done in 2.47 s
2020-06-12 12:50:28,763 - INFO - Starting 1 epoch...
2020-06-12 12:52:45,031 - INFO - logger set up
2020-06-12 12:52:45,032 - INFO - seed=718
2020-06-12 12:52:45,032 - INFO - #####
2020-06-12 12:52:45,032 - INFO - #####
2020-06-12 12:52:45,032 - INFO - Starting fold 0 ...
2020-06-12 12:52:45,032 - INFO - #####
2020-06-12 12:52:45,032 - INFO - #####
2020-06-12 12:52:45,185 - INFO - [load csv data] done in 0.15 s
2020-06-12 12:52:45,240 - INFO - [prepare validation data] done in 0.05 s
2020-06-12 12:52:45,803 - INFO - loading configuration file inputs/roberta-base/config.json
2020-06-12 12:52:45,804 - INFO - Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}

2020-06-12 12:52:45,804 - INFO - loading weights file inputs/roberta-base/pytorch_model.bin
2020-06-12 12:52:51,905 - INFO - [create model] done in 6.66 s
2020-06-12 12:52:51,905 - INFO - Starting 1 epoch...
2020-06-12 12:53:20,550 - INFO - logger set up
2020-06-12 12:53:20,550 - INFO - seed=718
2020-06-12 12:53:20,550 - INFO - #####
2020-06-12 12:53:20,550 - INFO - #####
2020-06-12 12:53:20,550 - INFO - Starting fold 0 ...
2020-06-12 12:53:20,550 - INFO - #####
2020-06-12 12:53:20,550 - INFO - #####
2020-06-12 12:53:20,705 - INFO - [load csv data] done in 0.15 s
2020-06-12 12:53:20,760 - INFO - [prepare validation data] done in 0.06 s
2020-06-12 12:53:21,334 - INFO - loading configuration file inputs/roberta-base/config.json
2020-06-12 12:53:21,335 - INFO - Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}

2020-06-12 12:53:21,337 - INFO - loading weights file inputs/roberta-base/pytorch_model.bin
2020-06-12 12:53:27,674 - INFO - [create model] done in 6.91 s
2020-06-12 12:53:27,674 - INFO - Starting 1 epoch...
2020-06-12 12:53:44,728 - INFO - logger set up
2020-06-12 12:53:44,728 - INFO - seed=718
2020-06-12 12:53:44,728 - INFO - #####
2020-06-12 12:53:44,728 - INFO - #####
2020-06-12 12:53:44,729 - INFO - Starting fold 0 ...
2020-06-12 12:53:44,729 - INFO - #####
2020-06-12 12:53:44,729 - INFO - #####
2020-06-12 12:53:44,883 - INFO - [load csv data] done in 0.15 s
2020-06-12 12:53:44,937 - INFO - [prepare validation data] done in 0.05 s
2020-06-12 12:53:45,501 - INFO - loading configuration file inputs/roberta-base/config.json
2020-06-12 12:53:45,502 - INFO - Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}

2020-06-12 12:53:45,503 - INFO - loading weights file inputs/roberta-base/pytorch_model.bin
2020-06-12 12:53:51,683 - INFO - [create model] done in 6.75 s
2020-06-12 12:53:51,684 - INFO - Starting 1 epoch...
2020-06-12 12:58:08,606 - INFO - save model at score=0.6889471892811131 on epoch=1
2020-06-12 12:58:08,606 - INFO - Starting 2 epoch...
2020-06-12 13:23:12,951 - INFO - logger set up
2020-06-12 13:23:12,951 - INFO - seed=718
2020-06-12 13:23:12,951 - INFO - #####
2020-06-12 13:23:12,951 - INFO - #####
2020-06-12 13:23:12,951 - INFO - Starting fold 0 ...
2020-06-12 13:23:12,951 - INFO - #####
2020-06-12 13:23:12,951 - INFO - #####
2020-06-12 13:23:13,102 - INFO - [load csv data] done in 0.15 s
2020-06-12 13:23:13,156 - INFO - [prepare validation data] done in 0.05 s
2020-06-12 13:23:13,719 - INFO - loading configuration file inputs/roberta-base/config.json
2020-06-12 13:23:13,720 - INFO - Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}

2020-06-12 13:23:13,720 - INFO - loading weights file inputs/roberta-base/pytorch_model.bin
2020-06-12 13:23:19,948 - INFO - [create model] done in 6.79 s
2020-06-12 13:23:19,948 - INFO - Starting 1 epoch...
2020-06-12 13:27:36,406 - INFO - save model at score=0.5859002066988838 on epoch=1
2020-06-12 13:27:36,406 - INFO - Starting 2 epoch...
2020-06-12 13:31:54,561 - INFO - save model at score=0.5931552128842777 on epoch=2
2020-06-12 13:31:54,562 - INFO - Starting 3 epoch...
2020-06-12 13:35:08,624 - INFO - logger set up
2020-06-12 13:35:08,624 - INFO - seed=718
2020-06-12 13:35:08,624 - INFO - #####
2020-06-12 13:35:08,624 - INFO - #####
2020-06-12 13:35:08,624 - INFO - Starting fold 0 ...
2020-06-12 13:35:08,624 - INFO - #####
2020-06-12 13:35:08,624 - INFO - #####
2020-06-12 13:35:08,778 - INFO - [load csv data] done in 0.15 s
2020-06-12 13:35:08,833 - INFO - [prepare validation data] done in 0.06 s
2020-06-12 13:35:09,404 - INFO - loading configuration file inputs/roberta-base/config.json
2020-06-12 13:35:09,405 - INFO - Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}

2020-06-12 13:35:09,406 - INFO - loading weights file inputs/roberta-base/pytorch_model.bin
2020-06-12 13:35:15,600 - INFO - [create model] done in 6.77 s
2020-06-12 13:35:15,600 - INFO - Starting 1 epoch...
2020-06-12 13:36:08,574 - INFO - logger set up
2020-06-12 13:36:08,574 - INFO - seed=718
2020-06-12 13:36:08,574 - INFO - #####
2020-06-12 13:36:08,574 - INFO - #####
2020-06-12 13:36:08,574 - INFO - Starting fold 0 ...
2020-06-12 13:36:08,574 - INFO - #####
2020-06-12 13:36:08,574 - INFO - #####
2020-06-12 13:36:08,727 - INFO - [load csv data] done in 0.15 s
2020-06-12 13:36:08,783 - INFO - [prepare validation data] done in 0.06 s
2020-06-12 13:36:09,348 - INFO - loading configuration file inputs/roberta-base/config.json
2020-06-12 13:36:09,348 - INFO - Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}

2020-06-12 13:36:09,349 - INFO - loading weights file inputs/roberta-base/pytorch_model.bin
2020-06-12 13:36:15,540 - INFO - [create model] done in 6.76 s
2020-06-12 13:36:15,540 - INFO - Starting 1 epoch...
2020-06-12 13:36:36,914 - INFO - logger set up
2020-06-12 13:36:36,914 - INFO - seed=718
2020-06-12 13:36:36,914 - INFO - #####
2020-06-12 13:36:36,914 - INFO - #####
2020-06-12 13:36:36,914 - INFO - Starting fold 0 ...
2020-06-12 13:36:36,915 - INFO - #####
2020-06-12 13:36:36,915 - INFO - #####
2020-06-12 13:36:37,068 - INFO - [load csv data] done in 0.15 s
2020-06-12 13:36:37,123 - INFO - [prepare validation data] done in 0.06 s
2020-06-12 13:36:37,686 - INFO - loading configuration file inputs/roberta-base/config.json
2020-06-12 13:36:37,686 - INFO - Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}

2020-06-12 13:36:37,687 - INFO - loading weights file inputs/roberta-base/pytorch_model.bin
2020-06-12 13:36:43,877 - INFO - [create model] done in 6.75 s
2020-06-12 13:36:43,877 - INFO - Starting 1 epoch...
2020-06-12 13:41:02,182 - INFO - save model at score=0.22761727256245767 on epoch=1
2020-06-12 13:41:02,182 - INFO - Starting 2 epoch...
2020-06-12 13:43:32,117 - INFO - logger set up
2020-06-12 13:43:32,117 - INFO - seed=718
2020-06-12 13:43:32,117 - INFO - #####
2020-06-12 13:43:32,117 - INFO - #####
2020-06-12 13:43:32,117 - INFO - Starting fold 0 ...
2020-06-12 13:43:32,117 - INFO - #####
2020-06-12 13:43:32,117 - INFO - #####
2020-06-12 13:43:32,270 - INFO - [load csv data] done in 0.15 s
2020-06-12 13:43:32,324 - INFO - [prepare validation data] done in 0.05 s
2020-06-12 13:43:32,889 - INFO - loading configuration file inputs/roberta-base/config.json
2020-06-12 13:43:32,890 - INFO - Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}

2020-06-12 13:43:32,892 - INFO - loading weights file inputs/roberta-base/pytorch_model.bin
2020-06-12 13:43:39,105 - INFO - [create model] done in 6.78 s
2020-06-12 13:43:39,105 - INFO - Starting 1 epoch...
2020-06-12 14:07:43,939 - INFO - logger set up
2020-06-12 14:07:43,939 - INFO - seed=718
2020-06-12 14:07:43,939 - INFO - #####
2020-06-12 14:07:43,939 - INFO - #####
2020-06-12 14:07:43,939 - INFO - Starting fold 0 ...
2020-06-12 14:07:43,939 - INFO - #####
2020-06-12 14:07:43,939 - INFO - #####
2020-06-12 14:07:44,092 - INFO - [load csv data] done in 0.15 s
2020-06-12 14:07:44,146 - INFO - [prepare validation data] done in 0.05 s
2020-06-12 14:07:44,721 - INFO - loading configuration file inputs/roberta-base/config.json
2020-06-12 14:07:44,721 - INFO - Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}

2020-06-12 14:07:44,722 - INFO - loading weights file inputs/roberta-base/pytorch_model.bin
2020-06-12 14:07:50,831 - INFO - [create model] done in 6.68 s
2020-06-12 14:07:50,831 - INFO - Starting 1 epoch...
2020-06-12 14:08:28,864 - INFO - logger set up
2020-06-12 14:08:28,864 - INFO - seed=718
2020-06-12 14:08:28,865 - INFO - #####
2020-06-12 14:08:28,865 - INFO - #####
2020-06-12 14:08:28,865 - INFO - Starting fold 0 ...
2020-06-12 14:08:28,865 - INFO - #####
2020-06-12 14:08:28,865 - INFO - #####
2020-06-12 14:08:29,017 - INFO - [load csv data] done in 0.15 s
2020-06-12 14:08:29,072 - INFO - [prepare validation data] done in 0.05 s
2020-06-12 14:08:29,636 - INFO - loading configuration file inputs/roberta-base/config.json
2020-06-12 14:08:29,637 - INFO - Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}

2020-06-12 14:08:29,638 - INFO - loading weights file inputs/roberta-base/pytorch_model.bin
2020-06-12 14:08:36,001 - INFO - [create model] done in 6.93 s
2020-06-12 14:08:36,002 - INFO - Starting 1 epoch...
2020-06-12 14:10:31,578 - INFO - logger set up
2020-06-12 14:10:31,579 - INFO - seed=718
2020-06-12 14:10:31,579 - INFO - #####
2020-06-12 14:10:31,579 - INFO - #####
2020-06-12 14:10:31,579 - INFO - Starting fold 0 ...
2020-06-12 14:10:31,579 - INFO - #####
2020-06-12 14:10:31,579 - INFO - #####
2020-06-12 14:10:31,730 - INFO - [load csv data] done in 0.15 s
2020-06-12 14:10:31,784 - INFO - [prepare validation data] done in 0.05 s
2020-06-12 14:10:32,347 - INFO - loading configuration file inputs/roberta-base/config.json
2020-06-12 14:10:32,348 - INFO - Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}

2020-06-12 14:10:32,349 - INFO - loading weights file inputs/roberta-base/pytorch_model.bin
2020-06-12 14:10:38,458 - INFO - [create model] done in 6.67 s
2020-06-12 14:10:38,458 - INFO - Starting 1 epoch...
2020-06-12 14:11:30,350 - INFO - logger set up
2020-06-12 14:11:30,351 - INFO - seed=718
2020-06-12 14:11:30,351 - INFO - #####
2020-06-12 14:11:30,351 - INFO - #####
2020-06-12 14:11:30,351 - INFO - Starting fold 0 ...
2020-06-12 14:11:30,351 - INFO - #####
2020-06-12 14:11:30,351 - INFO - #####
2020-06-12 14:11:30,508 - INFO - [load csv data] done in 0.16 s
2020-06-12 14:11:30,567 - INFO - [prepare validation data] done in 0.06 s
2020-06-12 14:11:31,153 - INFO - loading configuration file inputs/roberta-base/config.json
2020-06-12 14:11:31,154 - INFO - Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}

2020-06-12 14:11:31,155 - INFO - loading weights file inputs/roberta-base/pytorch_model.bin
2020-06-12 14:11:37,364 - INFO - [create model] done in 6.8 s
2020-06-12 14:11:37,364 - INFO - Starting 1 epoch...
2020-06-12 14:50:50,437 - INFO - logger set up
2020-06-12 14:50:50,437 - INFO - seed=718
2020-06-12 14:50:50,437 - INFO - #####
2020-06-12 14:50:50,437 - INFO - #####
2020-06-12 14:50:50,437 - INFO - Starting fold 0 ...
2020-06-12 14:50:50,437 - INFO - #####
2020-06-12 14:50:50,437 - INFO - #####
2020-06-12 14:50:50,589 - INFO - [load csv data] done in 0.15 s
2020-06-12 14:50:50,643 - INFO - [prepare validation data] done in 0.05 s
2020-06-12 14:50:51,206 - INFO - loading configuration file inputs/roberta-base/config.json
2020-06-12 14:50:51,207 - INFO - Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}

2020-06-12 14:50:51,208 - INFO - loading weights file inputs/roberta-base/pytorch_model.bin
2020-06-12 14:50:57,285 - INFO - [create model] done in 6.64 s
2020-06-12 14:50:57,285 - INFO - Starting 1 epoch...
2020-06-12 14:51:42,364 - INFO - logger set up
2020-06-12 14:51:42,364 - INFO - seed=718
2020-06-12 14:51:42,364 - INFO - #####
2020-06-12 14:51:42,364 - INFO - #####
2020-06-12 14:51:42,365 - INFO - Starting fold 0 ...
2020-06-12 14:51:42,365 - INFO - #####
2020-06-12 14:51:42,365 - INFO - #####
2020-06-12 14:51:42,516 - INFO - [load csv data] done in 0.15 s
2020-06-12 14:51:42,570 - INFO - [prepare validation data] done in 0.05 s
2020-06-12 14:51:43,135 - INFO - loading configuration file inputs/roberta-base/config.json
2020-06-12 14:51:43,135 - INFO - Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}

2020-06-12 14:51:43,136 - INFO - loading weights file inputs/roberta-base/pytorch_model.bin
2020-06-12 14:51:49,248 - INFO - [create model] done in 6.68 s
2020-06-12 14:51:49,248 - INFO - Starting 1 epoch...
2020-06-12 14:56:07,081 - INFO - save model at score=0.6839984391400111 on epoch=1
2020-06-12 14:56:07,081 - INFO - Starting 2 epoch...
2020-06-12 15:00:26,394 - INFO - save model at score=0.6883316690501786 on epoch=2
2020-06-12 15:00:26,394 - INFO - Starting 3 epoch...
2020-06-12 15:05:48,048 - INFO - logger set up
2020-06-12 15:05:48,048 - INFO - seed=718
2020-06-12 15:05:48,048 - INFO - #####
2020-06-12 15:05:48,048 - INFO - #####
2020-06-12 15:05:48,048 - INFO - Starting fold 0 ...
2020-06-12 15:05:48,048 - INFO - #####
2020-06-12 15:05:48,048 - INFO - #####
2020-06-12 15:05:48,200 - INFO - [load csv data] done in 0.15 s
2020-06-12 15:05:48,255 - INFO - [prepare validation data] done in 0.05 s
2020-06-12 15:05:48,818 - INFO - loading configuration file inputs/roberta-base/config.json
2020-06-12 15:05:48,819 - INFO - Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}

2020-06-12 15:05:48,820 - INFO - loading weights file inputs/roberta-base/pytorch_model.bin
2020-06-12 15:05:55,004 - INFO - [create model] done in 6.75 s
2020-06-12 15:05:55,004 - INFO - Starting 1 epoch...
2020-06-12 15:10:11,947 - INFO - save model at score=0.6941010769361928 on epoch=1
2020-06-12 15:10:11,947 - INFO - Starting 2 epoch...
2020-06-12 15:14:30,619 - INFO - save model at score=0.6965155108228543 on epoch=2
2020-06-12 15:14:30,619 - INFO - Starting 3 epoch...
2020-06-12 15:18:47,988 - INFO - best score is not updated while 1 epochs of training
2020-06-12 15:18:47,988 - INFO - Starting 4 epoch...
2020-06-12 15:23:05,415 - INFO - best score is not updated while 2 epochs of training
2020-06-12 15:23:05,415 - INFO - best score=0.6965155108228543 on epoch=2
2020-06-12 15:23:05,415 - INFO - [training loop] done in 1030.41 s
2020-06-12 15:23:05,420 - INFO - #####
2020-06-12 15:23:05,420 - INFO - #####
2020-06-12 15:23:05,421 - INFO - Starting fold 1 ...
2020-06-12 15:23:05,421 - INFO - #####
2020-06-12 15:23:05,421 - INFO - #####
2020-06-12 15:23:05,643 - INFO - [load csv data] done in 0.22 s
2020-06-12 15:23:05,696 - INFO - [prepare validation data] done in 0.05 s
2020-06-12 15:23:05,697 - INFO - loading configuration file inputs/roberta-base/config.json
2020-06-12 15:23:05,697 - INFO - Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}

2020-06-12 15:23:05,697 - INFO - loading weights file inputs/roberta-base/pytorch_model.bin
2020-06-12 15:23:08,145 - INFO - [create model] done in 2.45 s
2020-06-12 15:23:08,145 - INFO - Starting 1 epoch...
2020-06-12 15:27:26,240 - INFO - save model at score=0.6730559090412713 on epoch=1
2020-06-12 15:27:26,240 - INFO - Starting 2 epoch...
2020-06-12 15:31:44,295 - INFO - save model at score=0.686755045350411 on epoch=2
2020-06-12 15:31:44,295 - INFO - Starting 3 epoch...
2020-06-12 15:36:01,985 - INFO - save model at score=0.6937371808851778 on epoch=3
2020-06-12 15:36:01,985 - INFO - Starting 4 epoch...
2020-06-12 15:40:19,519 - INFO - save model at score=0.6939020878338847 on epoch=4
2020-06-12 15:40:19,519 - INFO - best score=0.6939020878338847 on epoch=4
2020-06-12 15:40:19,519 - INFO - [training loop] done in 1031.37 s
2020-06-12 15:40:19,522 - INFO - #####
2020-06-12 15:40:19,522 - INFO - #####
2020-06-12 15:40:19,522 - INFO - Starting fold 2 ...
2020-06-12 15:40:19,522 - INFO - #####
2020-06-12 15:40:19,522 - INFO - #####
2020-06-12 15:40:19,708 - INFO - [load csv data] done in 0.19 s
2020-06-12 15:40:19,802 - INFO - [prepare validation data] done in 0.09 s
2020-06-12 15:40:19,802 - INFO - loading configuration file inputs/roberta-base/config.json
2020-06-12 15:40:19,803 - INFO - Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}

2020-06-12 15:40:19,804 - INFO - loading weights file inputs/roberta-base/pytorch_model.bin
2020-06-12 15:40:22,385 - INFO - [create model] done in 2.58 s
2020-06-12 15:40:22,386 - INFO - Starting 1 epoch...
2020-06-12 15:44:40,250 - INFO - save model at score=0.6861820620639566 on epoch=1
2020-06-12 15:44:40,250 - INFO - Starting 2 epoch...
2020-06-12 15:48:58,275 - INFO - save model at score=0.6966907128161969 on epoch=2
2020-06-12 15:48:58,275 - INFO - Starting 3 epoch...
2020-06-12 15:53:15,620 - INFO - best score is not updated while 1 epochs of training
2020-06-12 15:53:15,620 - INFO - Starting 4 epoch...
2020-06-12 15:57:33,641 - INFO - best score is not updated while 2 epochs of training
2020-06-12 15:57:33,642 - INFO - best score=0.6966907128161969 on epoch=2
2020-06-12 15:57:33,642 - INFO - [training loop] done in 1031.26 s
2020-06-12 15:57:33,644 - INFO - #####
2020-06-12 15:57:33,644 - INFO - #####
2020-06-12 15:57:33,644 - INFO - Starting fold 3 ...
2020-06-12 15:57:33,644 - INFO - #####
2020-06-12 15:57:33,644 - INFO - #####
2020-06-12 15:57:33,788 - INFO - [load csv data] done in 0.14 s
2020-06-12 15:57:33,863 - INFO - [prepare validation data] done in 0.08 s
2020-06-12 15:57:33,864 - INFO - loading configuration file inputs/roberta-base/config.json
2020-06-12 15:57:33,864 - INFO - Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}

2020-06-12 15:57:33,864 - INFO - loading weights file inputs/roberta-base/pytorch_model.bin
2020-06-12 15:57:36,284 - INFO - [create model] done in 2.42 s
2020-06-12 15:57:36,284 - INFO - Starting 1 epoch...
2020-06-12 16:01:54,248 - INFO - save model at score=0.6961289645605093 on epoch=1
2020-06-12 16:01:54,248 - INFO - Starting 2 epoch...
2020-06-12 16:06:12,409 - INFO - best score is not updated while 1 epochs of training
2020-06-12 16:06:12,409 - INFO - Starting 3 epoch...
2020-06-12 16:10:30,918 - INFO - save model at score=0.6990368773238961 on epoch=3
2020-06-12 16:10:30,919 - INFO - Starting 4 epoch...
2020-06-12 16:14:48,975 - INFO - save model at score=0.7021846773911253 on epoch=4
2020-06-12 16:14:48,975 - INFO - best score=0.7021846773911253 on epoch=4
2020-06-12 16:14:48,975 - INFO - [training loop] done in 1032.69 s
2020-06-12 16:14:48,978 - INFO - #####
2020-06-12 16:14:48,978 - INFO - #####
2020-06-12 16:14:48,978 - INFO - Starting fold 4 ...
2020-06-12 16:14:48,978 - INFO - #####
2020-06-12 16:14:48,978 - INFO - #####
2020-06-12 16:14:49,149 - INFO - [load csv data] done in 0.17 s
2020-06-12 16:14:49,244 - INFO - [prepare validation data] done in 0.09 s
2020-06-12 16:14:49,245 - INFO - loading configuration file inputs/roberta-base/config.json
2020-06-12 16:14:49,246 - INFO - Model config RobertaConfig {
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "type_vocab_size": 1,
  "vocab_size": 50265
}

2020-06-12 16:14:49,246 - INFO - loading weights file inputs/roberta-base/pytorch_model.bin
2020-06-12 16:14:51,637 - INFO - [create model] done in 2.39 s
2020-06-12 16:14:51,637 - INFO - Starting 1 epoch...
2020-06-12 16:19:09,673 - INFO - save model at score=0.6929377559636928 on epoch=1
2020-06-12 16:19:09,674 - INFO - Starting 2 epoch...
2020-06-12 16:23:27,835 - INFO - save model at score=0.6986937656153438 on epoch=2
2020-06-12 16:23:27,835 - INFO - Starting 3 epoch...
2020-06-12 16:27:46,054 - INFO - save model at score=0.7012112992875744 on epoch=3
2020-06-12 16:27:46,054 - INFO - Starting 4 epoch...
2020-06-12 16:32:03,849 - INFO - save model at score=0.7042925016547285 on epoch=4
2020-06-12 16:32:03,850 - INFO - best score=0.7042925016547285 on epoch=4
2020-06-12 16:32:03,850 - INFO - [training loop] done in 1032.21 s
